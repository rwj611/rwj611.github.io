---
layout:     post
title:      "词向量之ELMO"
subtitle:   ""
date:       2020-11-10 12:00:00
author:     "rwj611"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - ai
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

#### 1. word embedding缺点
不能处理同义词


#### 2. 双向语言模型
给定N个token的序列: ($ t_1,t_2,t_3,t_4,...,t_N $)

前向语言模型:
$ p(t_1,t_2,t_3,...,t_N) = \prod_{k=1}^Np(t_k|t_1,t_2,...,t_{k-1}) $

后向语言模型:
$ p(t_1,t_2,t_3,...,t_N) = \prod_{k=1}^Np(t_k|t_{k+1},t_{k+2},...,t_N) $

前向语言模型预测$t_{k+1}$:
1. 首先计算上下文无关的token表示 $X_k^{LM}$ 
2. 将之传递通过前向LSTM的L层
3. 对于每个位置k, LSTM输出上下文相关的表示$\stackrel{\rightarrow}{h}_{k,j}^{LM}$
